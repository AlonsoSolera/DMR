# -*- coding: utf-8 -*-
"""DMR_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t_DVb9VqHHIfuBZGigdfWKXtZ4Em8Srv

# DMR Project

# Preload Data
"""

# Load data from Drive
from google.colab import drive
drive.mount('/content/drive')

# Import all libraries

from sklearn.tree import plot_tree
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import roc_curve, auc
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

import csv
import pandas as pd
import numpy as np
import re
import shap
import matplotlib.pyplot as plt
import seaborn as sns

!pip install lime -q

# Data Base Route
file_path = '/content/drive/MyDrive/DMR_Python/DATA/Data.xlsx'

# Load all sheets from Excel
df_charges = pd.read_excel(file_path, sheet_name='Charges')
df_other = pd.read_excel(file_path, sheet_name='Other data')
df_churn = pd.read_excel(file_path, sheet_name='Churn')

"""# Data Cleaning"""

# function to convert CamelCase to snake_case
def camel_to_snake_case(column_name):
    return re.sub(r'(?<!^)([A-Z])', r'_\1', column_name).lower()

# Function to edit names
def fix_column_name(column_name):
    column_name = column_name.replace('_i_d', '_id')
    column_name = column_name.replace('_t_v', '_tv')
    return column_name

# Apply both functions to column names
df_charges.columns = [fix_column_name(camel_to_snake_case(col)) for col in df_charges.columns]
df_other.columns = [fix_column_name(camel_to_snake_case(col)) for col in df_other.columns]
df_churn.columns = [fix_column_name(camel_to_snake_case(col)) for col in df_churn.columns]

# Verify column names
print("Column names in df_charges:", df_charges.columns)
print("Column names in df_other:", df_other.columns)
print("Column names in df_churn:", df_churn.columns)

# Check the info from each DataFrame
print("Information of df_charges:")
print(df_charges.info())
print("\nInformation of df_other:")
print(df_other.info())
print("\nInformation of df_churn:")
print(df_churn.info())

# Convert numeric columns in string format
df_charges['total_charges'] = pd.to_numeric(df_charges['total_charges'], errors='coerce')
df_charges['monthly_charges'] = pd.to_numeric(df_charges['total_charges'], errors='coerce')

# Check that the modifications were made correctly
print("Information of df_charges:")
print(df_charges.info())

# Eliminate doubles from Customer ID
df_charges = df_charges.drop_duplicates(subset='customer_id')
df_other = df_other.drop_duplicates(subset='customer_id')
df_churn = df_churn.drop_duplicates(subset='customer_id')

# Check Customer ID are unique values
assert df_charges['customer_id'].is_unique
assert df_other['customer_id'].is_unique
assert df_churn['customer_id'].is_unique

# Union of DataFrames through 'customer_id'
df_merged = df_charges.merge(df_other, on="customer_id", how="outer").merge(df_churn, on="customer_id", how="outer")
print("\nDimensions after the merge:", df_merged.shape)

# Check and identify null values from DataFrame combined
print("\nNumber of null values per column:")
print(df_merged.isnull().sum())

# Check that Customer IDs are without Total Charges
missing_total_charges = df_merged[df_merged["total_charges"].isnull()]["customer_id"]
print("Customer IDs without Total Charges:")
print(missing_total_charges)

# Check that Customer IDs are without Monthly Charges
missing_monthly_charges = df_merged[df_merged["monthly_charges"].isnull()]["customer_id"]
print("Customer IDs without Total Charges:")
print(missing_monthly_charges)

"""# Transformation"""

# Attach null values
df_merged['total_charges'].fillna(df_merged['total_charges'].median(), inplace=True)
df_merged['monthly_charges'].fillna(df_merged['monthly_charges'].median(), inplace=True)

# For categoric columns, attach with the mode
for col in df_merged.select_dtypes(include=['object', 'category']).columns:
    df_merged[col].fillna(df_merged[col].mode()[0], inplace=True)

# Check outliers in 'monthly_charges' using percentiles
q_low = df_merged['monthly_charges'].quantile(0.01)
q_hi  = df_merged['monthly_charges'].quantile(0.99)
df_merged = df_merged[(df_merged['monthly_charges'] >= q_low) & (df_merged['monthly_charges'] <= q_hi)]

# Change categoric columns to type 'category'
for col in df_merged.select_dtypes(include=['object']).columns:
    df_merged[col] = df_merged[col].astype('category')

# Visualize the first records from clean DataFrame
print("\nFirst records from clean DataFrame:")
print(df_merged.head())

"""# Testing/training"""

# Z-Score for outlier detection
from scipy import stats
z_scores = stats.zscore(df_merged['monthly_charges'])
threshold = 3
outliers = (abs(z_scores) > threshold)
df_cleaned = df_merged[(~outliers)]
print(f"Removed {outliers.sum()} outliers based on Z-score.")

# IQR-based outlier detection
Q1 = df_merged['monthly_charges'].quantile(0.25)
Q3 = df_merged['monthly_charges'].quantile(0.75)
IQR = Q3 - Q1

# Define upper and lower limits from outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# filter oultilers
df_cleaned = df_merged[(df_merged['monthly_charges'] >= lower_bound) & (df_merged['monthly_charges'] <= upper_bound)]
print(f"Removed {len(df_merged) - len(df_cleaned)} outliers based on IQR.")

# Convert tenure = 0 to 1 (avoid division by zero)
df_merged['tenure'] = df_merged['tenure'].replace(0, 1)

# Create new features
df_merged['avg_monthly_charges'] = df_merged['total_charges'] / df_merged['tenure']
df_merged['auto_payment'] = df_merged['payment_method'].isin(['Credit card (automatic)', 'Bank transfer (automatic)']).astype(int)

# Encode categorical features
categorical_features = ['contract', 'internet_service', 'payment_method']
df_encoded = pd.get_dummies(df_merged, columns=categorical_features, drop_first=True)

# Convert target variable to binary
df_encoded['churn'] = df_encoded['churn'].map({'Yes': 1, 'No': 0})

# Split data into train & test
X = df_encoded.drop(columns=['customer_id', 'churn'])
y = df_encoded['churn']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Convert categorical features to numerical using One-Hot Encoding
categorical_features = ['gender', 'partner', 'dependents', 'phone_service',
                        'multiple_lines', 'internet_service', 'online_security',
                        'online_backup', 'device_protection', 'tech_support',
                        'streaming_tv', 'streaming_movies', 'contract',
                        'paperless_billing', 'payment_method']

df_encoded = pd.get_dummies(df_merged, columns=categorical_features, drop_first=True)

# Convert target variable to binary
df_encoded['churn'] = df_encoded['churn'].map({'Yes': 1, 'No': 0})

# Split data into train & test
X = df_encoded.drop(columns=['customer_id', 'churn'])
y = df_encoded['churn']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Now, apply SMOTE without categorical data issues
from imblearn.over_sampling import SMOTE

# Apply SMOTE for class balancing
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

print("Before SMOTE:", X_train.shape, y_train.shape)
print("After SMOTE:", X_train_resampled.shape, y_train_resampled.shape)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# Train Random Forest model
rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight="balanced")
rf.fit(X_train_resampled, y_train_resampled)

# Predictions
y_pred = rf.predict(X_test)
y_proba = rf.predict_proba(X_test)[:, 1]

# Model Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred))
print("ROC AUC Score:", roc_auc_score(y_test, y_proba))
print(classification_report(y_test, y_pred))

# SHAP Values Calculation
explainer = shap.TreeExplainer(rf)
shap_values = explainer.shap_values(X_test)

# Summary plot to show feature importance
shap.summary_plot(shap_values[:, :, 1] , X_test, plot_type = "bar")

important_features = X_train_resampled.columns[shap_values[1].mean(axis=0).argsort()[::-1][:10]]
X_train_filtered = X_train_resampled[important_features]
X_test_filtered = X_test[important_features]

# visualization: churn rate vs. contract type
plt.figure(figsize=(8, 5))
sns.barplot(x=df_merged['contract'], y=df_merged['churn'].map({'Yes': 1, 'No': 0}))
plt.title("Churn Rate by Contract Type")
plt.ylabel("Churn Probability")
plt.xlabel("Contract Type")
plt.show()

# Get feature importances
feature_importances = pd.DataFrame({'Feature': X_train.columns, 'Importance': rf.feature_importances_})
feature_importances = feature_importances.sort_values(by='Importance', ascending=False)

# Plot feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importances, palette='viridis')
plt.xlabel("Feature Importance")
plt.ylabel("Feature Name")
plt.title("Random Forest Feature Importance")
plt.show()

# Extract a single tree from the Random Forest
plt.figure(figsize=(35, 24))
plot_tree(rf.estimators_[0], feature_names=X_train.columns, class_names=["No Churn", "Churn"], filled=True, max_depth=3, fontsize = 12)
plt.title("Decision Tree from Random Forest", fontsize = 20)
plt.show()

# Predict on the test set
y_pred = rf.predict(X_test)

# Generate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["No Churn", "Churn"])
disp.plot(cmap="Blues")
plt.title("Random Forest Confusion Matrix")
plt.show()

# Compute ROC curve
fpr, tpr, _ = roc_curve(y_test, y_proba)

# Plot ROC curve
plt.figure(figsize=(8, 8))
plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {roc_auc_score(y_test, y_proba):.2f})", color="blue")
plt.plot([0, 1], [0, 1], "k--", label="Random Model (AUC = 0.5)")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.grid()
plt.show()

# Compute Precision-Recall curve
precision, recall, _ = precision_recall_curve(y_test, y_proba)

# Plot Precision-Recall curve
plt.figure(figsize=(8, 8))
plt.plot(recall, precision, marker='.', color="green", label="Precision-Recall Curve")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.legend()
plt.grid()
plt.show()

"""# MODEL PERFORMANCE ENHANCEMENT

Hyperparameter Tuning (GridSearchCV for Random Forest)
"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

param_grid = {
    'n_estimators': [100, 300, 500],
    'max_depth': [1, 3, None],
    'min_samples_split': [2, 3, 4],
    'min_samples_leaf': [1, 2, 4]
}

rf = RandomForestClassifier(random_state=42, class_weight="balanced")
grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)
grid_search.fit(X_train_resampled, y_train_resampled)

best_rf = grid_search.best_estimator_
print("Best Parameters:", grid_search.best_params_)

# Use the best parameters found by GridSearchCV
best_params = {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}

# Train Random Forest model with the best parameters
rf = RandomForestClassifier(random_state=42, class_weight="balanced", **best_params)
rf.fit(X_train_resampled, y_train_resampled)

# Predictions
y_pred = rf.predict(X_test)
y_proba = rf.predict_proba(X_test)[:, 1]

# Model Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred))
print("ROC AUC Score:", roc_auc_score(y_test, y_proba))
print(classification_report(y_test, y_pred))

"""Other Models for Comparison (XGBoost & Logistic Regression)"""

# Train XGBoost
xgb = XGBClassifier(eval_metric="logloss", random_state=42)  # Removed use_label_encoder
xgb.fit(X_train_resampled, y_train_resampled)
xgb_pred = xgb.predict(X_test)
print("XGBoost ROC AUC:", roc_auc_score(y_test, xgb.predict_proba(X_test)[:,1]))

# Preprocess Data for Logistic Regression
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_resampled)
X_test_scaled = scaler.transform(X_test)

# Train Logistic Regression
log_reg = LogisticRegression(max_iter=500, solver="lbfgs", random_state=42)  # Increased max_iter
log_reg.fit(X_train_scaled, y_train_resampled)
log_reg_pred = log_reg.predict(X_test_scaled)
print("Logistic Regression ROC AUC:", roc_auc_score(y_test, log_reg.predict_proba(X_test_scaled)[:,1]))

"""ROC curve comaprison: XGBoost vs. Logistic Regression"""

xgb_probs = xgb.predict_proba(X_test)[:, 1]
xgb_fpr, xgb_tpr, _ = roc_curve(y_test, xgb_probs)
xgb_auc = auc(xgb_fpr, xgb_tpr)

# Compute ROC curve and AUC for Logistic Regression
log_reg_probs = log_reg.predict_proba(X_test_scaled)[:, 1]
log_reg_fpr, log_reg_tpr, _ = roc_curve(y_test, log_reg_probs)
log_reg_auc = auc(log_reg_fpr, log_reg_tpr)

# Plot ROC Curves
plt.figure(figsize=(8, 6))
plt.plot(xgb_fpr, xgb_tpr, label=f'XGBoost (AUC = {xgb_auc:.3f})', color='blue', lw=2)
plt.plot(log_reg_fpr, log_reg_tpr, label=f'Logistic Regression (AUC = {log_reg_auc:.3f})', color='red', lw=2)
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Guessing')

# Labels and Legend
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison: XGBoost vs Logistic Regression')
plt.legend(loc='lower right')
plt.grid()
plt.show()

"""# BUSINESS IMPACT ANALYSIS

Customer Segmentation (K-Means Clustering)
"""

kmeans = KMeans(n_clusters=3, random_state=42)
X_train_resampled['cluster'] = kmeans.fit_predict(X_train_resampled)

plt.scatter(X_train_resampled.iloc[:, 0], X_train_resampled.iloc[:, 1], c=X_train_resampled['cluster'], cmap='viridis')
plt.title("Customer Segmentation (K-Means)")
plt.show()

""" Economic Impact of Churn"""

average_credit_value = 5000  # Assuming each costumer owes $5000 (taking into consideration a little bit more than the average of the total charges of the customers)
lost_revenue = df_encoded[df_encoded['churn'] == 1].shape[0] * average_credit_value
print(f"Estimated Lost Revenue Due to Churn: ${lost_revenue:,.2f}")

# Export clean DataFrame to csv format
df_merged.to_csv("DMR_clean_data.csv", index=False)

"""# End of the code"""